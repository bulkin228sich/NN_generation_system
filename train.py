import time
import torch
import torch.nn as nn
import os
from datetime import datetime
from tqdm import tqdm
from load_dataset import *
from schedule import models_train
class EarlyStopping:
    def __init__(self, patience=15, delta=0.0):
        self.patience = patience
        self.delta = delta
        self.best_loss = None
        self.counter = 0
        self.should_stop = False

    def step(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0


# ===== 6. –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ =====
def train_model(model, model_name, train_losses_models,  epochs=100, lr=0.0001):
    early_stopper = EarlyStopping(patience=15)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)
    criterion = nn.MSELoss()
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
    save_dir = os.path.join("saved_models", model_name)
    os.makedirs(save_dir, exist_ok=True)

    best_val_loss = float("inf")
    train_losses, val_losses = [], []

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    log_path = os.path.join(save_dir, "training_log.txt")
    with open(log_path, "w") as log_file:
        log_file.write(f"Training log for model: {model_name}\n")
        log_file.write(f"Started at: {datetime.now()}\n\n")

    # –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
    total_start_time = time.time()

    for epoch in range(epochs):
        epoch_start_time = time.time()
        model.train()
        train_loss = 0

        for x, y in train_loader:
            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)
            optimizer.zero_grad()
            pared = model(x)
            loss = criterion(pared, y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(device), y.to(device)
                pared = model(x)
                val_loss += criterion(pared, y).item()

        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        scheduler.step(avg_val_loss)

        epoch_time = time.time() - epoch_start_time

        # log_message = (f"Epoch {epoch + 1}/{epochs} | "
        #                f"Train Loss: {avg_train_loss:.6f} | "
        #                f"Val Loss: {avg_val_loss:.6f} | "
        #                f"Time: {epoch_time:.2f} sec")
        time.sleep(0.01)
        #tqdm.write(log_message)
        time.sleep(0.01)
        #with open(log_path, "a") as log_file:
            #log_file.write(log_message + "\n")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_path = os.path.join(save_dir, f"best_{model_name}.pt")
            torch.save(model.state_dict(), best_model_path)
            #print(f"\nüíæ best model saved: {avg_val_loss} {epoch+1}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å —ç–ø–æ—Ö–∏
        epoch_model_path = os.path.join(save_dir, f"model_epoch_{epoch+1}.pt")
        torch.save(model.state_dict(), epoch_model_path)

        early_stopper.step(avg_val_loss)
        if early_stopper.should_stop:
            #print("üõë Early stopping triggered!")
            break

    total_training_time = time.time() - total_start_time

    # # –†–∞—Å—á–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–æ–≥–æ loss –Ω–∞ 2 000 000 –¥–∞–Ω–Ω—ã—Ö
    # current_train_samples = 100000
    # future_samples = 2000000
    # if len(train_losses) >= 2:
    #     # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –¥–≤–µ —ç–ø–æ—Ö–∏
    #     loss1 = train_losses[-2]
    #     loss2 = train_losses[-1]
    #
    #     # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ–±—ã –∑–Ω–∞—á–µ–Ω–∏—è —Å—ç–º–ø–ª–æ–≤ –±—ã–ª–∏ —Ä–∞–∑–Ω—ã–µ
    #     earlier_samples = current_train_samples // 2  # –¥–æ–ø—É—Å—Ç–∏–º, –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º
    #     current_samples = current_train_samples
    #
    #     if current_samples == earlier_samples:
    #         k = 0.1  # –µ—Å–ª–∏ –Ω–µ—Ç —Ä–∞–∑–Ω–∏—Ü—ã, —Ñ–∏–∫—Å–∏—Ä—É–µ–º –º–∞–ª–µ–Ω—å–∫–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
    #     else:
    #         # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å–Ω–∏–∂–µ–Ω–∏—è –ª–æ—Å—Å–∞
    #         k = np.log(loss1 / loss2) / (np.log(current_samples / earlier_samples + 1e-8))
    #
    #     # –¢–µ–ø–µ—Ä—å –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –±—É–¥—É—â–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö
    #     predicted_loss = train_losses[-1] * (current_samples / future_samples) ** k
    #
    #     with open(log_path, "a") as log_file:
    #         log_file.write(f"\nPrediction:\n")
    #         log_file.write(f"Estimated Train Loss at {future_samples} samples: {predicted_loss:.8f}\n")
    #         predicted_loss = round(predicted_loss, 3)
    #         print(f"Estimated Train Loss at {future_samples} samples: {predicted_loss:.8f}\n")
    # else:
    #     with open(log_path, "a") as log_file:
    #         log_file.write("\nNot enough epochs to estimate future loss.\n")
    #
    # with open(log_path, "a") as log_file:
    #     log_file.write(f"\nTotal training time: {total_training_time:.2f} sec\n")
    #     print(f"\nTotal training time: {total_training_time:.2f} sec\n")
    #     log_file.write(f"Finished at: {datetime.now()}\n")
    #
    # #–ì—Ä–∞—Ñ–∏–∫
    # models_train(train_losses, val_losses, model_name, save_dir)
    #
    # train_losses_models.append([train_losses, f"{model_name}"])

